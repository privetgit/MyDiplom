\newpage

\section{Конструкторская часть}

\input{11_construct}

\subsection{Проектирование подсистемы}
\subsubsection{Разработка структуры подсистемы}
\paragraph{Определение состава компонентов}

Исходя из анализа функций структурно в подсистеме можно выделить следующие основные части:
\begin{itemize}
\item \textbf{модуль обработки входных данных} (преобразует входные данные в удобоваримый вариант для последующей обработки);
\item \textbf{модуль компьютерного зрения }(позволяет обрабатывать изображения и производить их анализ для построения визуальной одометрии);
\item \textbf{модуль визуальной одометрии} (высчитывает перемещение и угол поворота камеры на основе последовательности изображений);
\item \textbf{модуль обработки данных с инерционных приборов} (производит математическую обработку показаний датчиков и на ее основе вычисляет перемещение объекта);
\item \textbf{модуль сопоставления  и вывода данных} (сравнивает показания двух предыдущих модулей и на их основе выводит наиболее правдоподобное положение объекта).
\end{itemize}

\paragraph{Определение структуры компонентов}
Согласно общепринятой терминалогии система состоит из подсистем, а те в свою очередь из модулей. Таким образом разрабатываемая подсистема состоит из следующих модулей:

\begin{itemize}
\item обработки входных данных;
\item обработки выходных данных;
\item визульной одометрии;
\item компьютерного зрения;
\item настроек;
\item одометрии на основе инерционных устройств.
\end{itemize}

Графически архитектура подсистемы представлена на рисунке~\ref{pic:acrhitec}.

\begin{figure}[!htb]
\center{\includegraphics[width=0.8\linewidth]{pics/achitecture.png}}
\caption{Структура компонентов.}
\label{pic:acrhitec}
\end{figure}

\textbf{Назначение модулей}

\begin{itemize}
\item \textbf{Модуль обработки входных данных} - нормализует входные данные и аккумулирует их, в случае слишком высокой частоты их поступления.
\item \textbf{Модуль обработки выходных данных} - сравниевает данные, полученные независимо в модуле визуальной одометрии и в модуле обработки данных с ИИУ, и принимает решение об их корректности на основе определенных сигнальных показателей (см. пункт~\ref{item:outputCorrect}).
\item \textbf{Модуль визуальной одометрии} – вычисляет перемещения на основе видеоряда. Для обработки видео использует модуль компьютерного зрения. 
\item \textbf{Модуль компьютерного зрения} – реализует стандартные алгоритмы компютеного зрения, такие как вычисление оптического потока, поиск опорных точек на изображении, трансформация и измениние изображений.
\item \textbf{Модуль одометрии на основе инерционных устроств} – производит расчеты перемещения обхекта на основе полученных данныз с гироскопов и акселерометров.
\item \textbf{Модуль настроек} - содержит в себе данные об используемом оборудовании и его характеристиках. Эти данные используются в модулях одометрии. 
\end{itemize}

\paragraph{Описание процессов}
В ходе работы всей подсистемы протекает множество процессов по обработке и преобразованию информации. Выделим ключевые из них.

\textbf{Визуальная одометрия.}

В данной работе делается упор на создание визуальной одометрии на основе одной бытовой камеры и относительно слабых вычислительных устройств. Такое решение требует соблюдения двух условий, которые приемлемы при создании большинства современных мобильных роботов:
\begin{enumerate}
\item перемещение происходит по земле или другой горизонтальной плоскости;
\item камера жестко закреплена относительно самого носителя.
\end{enumerate}

При соблюдении этих условий работу визуальной одометрии можно описать в 8 шагов\cite{visaulOdometryMy}.
\begin{enumerate}
\item Исправление изображения для исключения искажения линз.
\item Вычисление оптического потока для кадра.
\item Проверка полученных векторов смещения, исключение движущихся в кадре объектов, исключение ошибочных векторов.
\item Разделение всего оптического потока на две части: «наземную» и «небесную».
\item В «небесной» части перейти к цилиндрической системе координат и высчитать угол поворота относительно двух последних кадров, определяя тем самым угол поворота $\theta$.
\item В «наземной» части выделить векторы $(u,v)$ из оптического потока  и вычислить перемещение в плоскости x-y , получив вектор $(x, y)$.
\item Прибавить $(x, y, \theta)$ к изначальному положению объекта $(X, Y, \Theta)$, получив новое положение объекта.
\item Перейти к шагу 1 для следующего кадра. Периодически обновлять ключевые точки.
\end{enumerate}
	
В общем виде процесс вычисления оптического потока представлен на рисунке~\ref{pic:visOdometryProc}.

\begin{figure}[!htb]
\center{\includegraphics[width=0.8\linewidth]{pics/visualOdometryProcess.png}}
\caption{Графическое представление процесса визуальной одометрии}
\label{pic:visOdometryProc}
\end{figure}

\textbf{Обработка данных с инерционных измерительных устроств.}

Обработка данных с инерционных устройств представляет собой несколько операций интегрирования полученных данных. 
Рассмотрим переход от ускорений к перемещению.

Пусть на выходе акселерометра снимаются ускорения $Z_a$, $Y_a$ и $Z_a$, которые показывают ускрения по осям $X$, $Y$, $Z$.

Как мы знаем из курса физики, $ \int_a^b a(t)dt = V(t)+C $.
В то же время $ \int_a^b V(t)dt = S(t)+C $. Таким образом получаем:
$$
\iint_a^b a(t)dt = \int_a^b V(t) + C_v = S(t) + C_v \cdot t + C_s 
$$.

Так как $С_s$ в нашем случае, это начальное положение носителя, то его мы принимаем = 0. Отсюда получаем формулу:
$$
\iint_a^b a(t)dt = \int_a^b V(t) + C_v = S(t) + C_v \cdot t .
$$

Однако все упрощается, если период замеров настолько мал, что ускорение можно считать равномерным. Тогда формулы преобретают следующий вид:
$$ V_t = V_{t-1} + a_t \cdot t$$
$$ S_t = S_{t-1} + V_t \cdot t $$

Графически это можно выразить тремя графиками (рис.~\ref{pic:integral}): показывающими рассчитанное значения пермещения и сокрости в зависимости от времени. 

\begin{figure}[!htb]
\center{\includegraphics[width=0.8\linewidth]{pics/integrals.png}}
\caption{Пример интегрирования ускорения для получения перемещения}
\label{pic:integral}
\end{figure}


Производя подобные математические вычисления для координат $X$ и 
$Y$ получаем перемещения по этим координатам. Причем мы можем опускать финальную константу интегрирования, так как нам необходимо определить не пройденный путь от начала движения, а именно путь, пройденный за последнний временной отрезок. 

Аналогично обрабатываются данные с акселерометров, с одинм лишь отличием - акселерометры могут предоставлять сразу угловую скорость, что позволяет производить лишь одно интегрирование.


\paragraph{Математическое обеспечение}\label{math}

В рамках данной работы многие процессы основываются на фундаментальных исследованиях в области компьютерного зрения и обработки изображений. Далее рассматриваются основные из них.

\textbf{Поиск ключевых точек}

Есть много видов местных ключевых точек, которые можно отслеживать. Для начала стоит рассмотреть что из себя представляет ключевая точка сама по себе (ключевая особенность изображения). Очевидно, что если мы выбираем точку на большой однотонной стене, то будет не легко найти, что же точку в следующем кадре из видео.

Если все точки на стене могут быть одинаковыми или даже очень похожи, то у нас будет мало шансов отслеживания этой точки в последующих кадрах. С другой стороны, если мы выберем точку, которая является уникальной, то у нас будут довольно хорошие шансы снова найти эту точку. На практике, точка или черта, которая мы выбираем в качестве клюевой должны быть уникальным или почти уникальны, и должна быть параметризуемой таким образом, чтобы ее можно было отличить от других точек на изображении.

Возвращаясь к нашей аналогии с большой пустой стеной, мы могли бы попытаться искать точки, которые имеют некоторые значительные изменения в окрестности, например, большую производную. На практике оказывается, что этого не достаточно. Точка, в которой большое значение производной, может находиться на какой-то линии, но все точки на этой линии будут иметь такую же или близкую производную.

Однако, если высокие значения производные наблюдаются в двух ортогональных направлениях, то можно надеяться, что эта точка будет уникальной. По этой причине, такие особенности на изображении называются углами. Очевидно, углы - не края - это точки, которые содержат достаточно информации, чтобы ее нашли на последующих кадрах.

Определение углов опирается на матрице производных второго порядка 
$(\partial^2 x, \partial^2 y, \partial x \partial y)$ интенсивностей изображения. Эта терминология происходит от матрицы Гессе вокруг точки, которая определяется в двух измерениях следующим образом:

$$ 
H(p) = 
\begin{bmatrix} 
	\frac{\partial^2 I}{\partial x^2} & \frac{\partial^2 I}{\partial x \partial y}  \\ 
 	 \frac{\partial^2 I}{\partial x \partial y} & \frac{\partial^2 I}{\partial y^2} 
\end{bmatrix}
$$

Такие углы, по определению Харриса, места на изображении, где автокорреляционная матрица вторых производных имеет два больших собственных значения. По сути, это означает, что есть текстуры или грани , идущие, по крайней мере, в двух отдельных направлениях, сосредоточенные вокруг такой точки, так же, как реальные углы имеют по крайней мере два ребра, сходящихся в точку. Использование вторых производных позволяет точно определить особенности, потому что они не отвечают единым градиентами(так как при первой производной равной константе, вторая будет равна нулю). Это определение имеет дополнительное преимущество, в том, что когда мы рассматриваем только собственных значений автокорреляционной матрицы, мы рассматриваем величины, инвариантные также к вращению, что является важным, потому что объекты, которые мы отслеживаем могут вращаться, а также перемещаться. Следует также отметить, что эти два собственных значения делают больше, чем просто определяют, является ли точка перспективной для обслеживания - они также обеспечивают идентифицирующую роль для точки.

В используемом в данной работе пакете компьютерного зрения используется функция  $cvGoodFeaturesToTrack()$.
\begin{verbatim}
void cvGoodFeaturesToTrack(
        const CvArr* image,
        CvArr* eigImage, CvArr* tempImage,
        CvPoint2D32f* corners,
        int* cornerCount,
        double qualityLevel,
        double minDistance,
        const CvArr* mask=NULL,
        int blockSize=3,
        int useHarris=0,
        double k=0.04 
);
\end{verbatim}
Эта функция вычисляет воторые производные для точек и их собственные значения. Результатом работы функции является список точек, которые являются хорошими кандидатами для ключевых точек.

\textbf{Оптический поток}

Задача вычисления оптического потока можно сформулировать следующим образом: оценить движение между двумя кадрами не имея никакой информации о происходящем, кроме самих кадров\cite{OpenCVBook}. 

Вычисля оптический поток, мы можем связать для каждого пикселя определить некий ветктор смешения, которые будут показывать куда переместился пиксель между предыдущим кадром и текущим кадром. Такой  подход обычно называют плотным оптическим потоком, который определяет перемещение каждого пикселя. Метод Хорн-Шунка (Horn-Schunck) вычисляет именно такой оптический поток. В его основе лежит один, казалось бы, простой принцип - просто пытаться найти наиболее похожий пиксель на следующем кадре в пределах какого-то окна вокруг исходного пикселя.

На практике расчет плотного оптического потока затруднителен. Рассмотрим движение белого листа бумаги. Многие из белых пикселей в предыдущем кадре просто остаются белыми на следующих. Изменения будут лишь на границах листа, и то, только вокруг гарниц перпендикулярных движению. В результате необходимо прибегать к различным математическим приемам, что сказывается на увеличении ресурсоемкости операции в целом. 

Это приводит нас к альтернативному варианту, выборочный оптического потока. Алгоритмы такого рода опираются на некоторые средства определения заранее подмножество точек, которые должны быть отслежены. Если эти точки имеют определенные свойства, такие как свойства ключевых особенностей, обсуждаемые ранее, тогда отслеживая будет относительно точным и надежными. Для многих практических случаев, вычислительная стоимость выборочного оптичкского потока намного меньше, чем у плотного оптического потока, поэтому последнему отводится только академический интерес.

Рассмотрим наиболее популярный метод вычисления выборочного оптического потока - метод Лукаса-Канаде (Lucas-Kanade). Этот метод также имеет реализацию, которая работает с пирамидами изображений, что позволяет нам отслеживать быстрые движения. В данной работе используется именно он, так как он обладает наиболее низкой вычислительной сложностью. 

{\large \textit{\textbf{Метод Лукаса-Канаде}}}
\label{label:lucas-kanade-math}

Метод (алгоритм) Лукаса - Канаде (ЛК) создавался в 1981 году и первоначально задумывался для вычисления плотного оптического потока. Тем не менее, Алгоритм работал и с любым количеством точек для отлеживания, что позволило использовать его в столь важных выборочных оптических потоках. 

Алгоритм ЛК может быть применен для определнного числа точек, потому что он опирается только на локальной информации о точке, которая является производной в некотором маленьком окне вокруг каждой из ключевых точек. Недостатком использования небольших локальных окон в ЛК является то, что большие смещения могут перемещать точки за пределы таких локального окон, что приведет к невозможности их нахождения. Эта проблема привела к разработке "пирамидальной" версии алгоритма, которая составляет пирамиду из нескольких копий изображения разного размера, после чего вычисляет оптический поток, начиная с самого высокого уровня в пирамиде изображений, постепенно опускаясь по уровням для более высокой точности. 

\begin{figure}[!htb]
\center{\includegraphics[width=0.8\linewidth]{pics/pyrLK.png}}
\caption{Графическое представление работы "пирамидальной" версии алгоритма ЛК}
\label{pic:pyrLK}
\end{figure}

\textbf{Принцип работы алгоритма}

Основная идея алгоритма ЛК основывается на трех предположениях. 
\begin{enumerate}
\item \textit{Яркость постояна.} Предполагается, что пиксель не меняет внешний вид, переходя от кадра к кадру. Для изображений в градациях серого (для случая с цветными изображениями есть более строгое допущение, но оно сводится к этому) это означает, что яркость пикселя не меняется, при его отслеживании от кадра к кадру. 
\item \textit{Малые сдвиги}. Изображение двигается медленно во времени. На практике это означает, что объект от кадра к кадру сдвигается незанчительно.
\item \textit{Пространственная когерентность.} Соседние точки в кадре принадлежат к одной и той же поверхности и имеют аналогичные смещения.
\end{enumerate}

Из первого допущения следует:
$$
f(x, t) \equiv I(x(t), t) = I(x(t+dt), t + dt)
$$

Отсюда следует, что интенсивность отслеживаиваемых пикселей не измененяется с течением времени:
$$
\frac{\partial f(x)}{\partial t} = 0
$$

Второе предположение, по существу, означает, что движения от кадра к кадру крайне малы. Другими словами, мы можем рассматривать это изменение как аппроксимацию производной от интенсивности по времени. Чтобы понять последствия этого предположения, рассмотрим сначала случай одного пространственного измерения.
Запишем уравнение яркости $F(x,t)$ с учетом зависимости $x$ от $t$ и применим правило частного дифференцирования:
$$
\underbrace{\frac{\partial I }{\partial x} \Bigr|_t}_{I_x} 
\underbrace{\left( \frac{\partial x }{\partial t} \right)}_{V} + 
\underbrace{\frac{\partial I }{\partial t} \Bigr|_{x(t)}}_{I_t} =
 0,
$$

где $I_x$ является пространственной производной по первому изображению, $I_t$ это производная между изображениями в течение долгого времени, и $V$ -скорость, которую мы ищем. Таким образом, мы приходим к простому уравнению для скорости оптического потока в простой одномерной случае:

$$ V = \frac{I_t}{I_x} $$

Рассмотрим стандартную задачу в одномерном случае. На рисунке~\ref{pic:OptFlow1D} представлена ее графическая иллюстрация. Кривая $I(x, t)$ изображает некую грань, слева от которой значения интенсивности высоки, а справа - низкие. Необходимо определить как сдвинулась это грань на следующем кадре. 

\begin{figure}[!htb]
\center{\includegraphics[width=0.8\linewidth]{pics/optFlow1D.png}}
\caption{Графическое представление задачи нахождения оптического потока в одномерном случае.}
\label{pic:OptFlow1D}
\end{figure}

На рисунке~\ref{pic:OptFlow1DSolve} показано как можно решить такую задачу. При учете двух первых предположений  получим следующее:
$$
I_x = \frac{\partial I}{\partial x} \Bigr|_t \; и \;
I_t = \frac{\partial I}{\partial t} \Bigr|_{x=p} \Rightarrow 
\vec{V} \approx - \frac{I_t}{I_x}
$$
\begin{figure}[!htb]
\center{\includegraphics[width=0.8\linewidth]{pics/OptFlow1DSolve.png}}
\caption{Графическое представление решения задачи нахождения оптического потока в одномерном случае.}
\label{pic:OptFlow1DSolve}
\end{figure}

Теперь рассмотрим двумерный случай. Будем обозначть $u_y$ скорость вдоль оси $Y$, а $u_x$ -вдоль $X$:
$$ I_xu +I_yv+I_t = 0 \equiv 
\frac{\partial I}{\partial t} + 
u_x \cdot \frac{\partial I}{\partial x} + 
u_y \cdot \frac{\partial I}{\partial y} = 0 
$$

Полученное уравнение, говорит нам о том, что сумма частных производных должны быть равна нулю. Но вознакает проблема - уравнение у нас одно, а неизвестных в нем два: $u_x$ и $u_y$.

Воспользовавшись третьим предположением, о том, что соседние пиксели смещаются на одинаковое расстояние, запишем это же уравнение для окна 5х5 пикселей, получив 25 уравнений. Очевидно, что 3 допущение не всегда верно, поэтому в общем случае система не имеет решения поэтому перейдем к минимизации ошибки:

$$
E(u_x, u_y) = \sum \limits_{i,j} g(x_i, y_i) 
\left[ \frac{\partial I}{\partial t} + 
u_x \cdot \frac{\partial I}{\partial x} + 
u_y \cdot \frac{\partial I}{\partial y} \right]^2
$$

Здесь $g$ — это функция, определяющая весовые коэффициенты для пикселей. Самые распространенный вариант — двумерная гауссиана, которая дает наибольший вес центральному пикселю и все меньший по мере удаления от центра (см. рис.~\ref{pic:Gaussian2d}).

\begin{figure}[!htb]
\center{\includegraphics[width=0.8\linewidth]{pics/Gaussian2d.png}}
\caption{Двумерная гауссиана}
\label{pic:Gaussian2d}
\end{figure}

Чтобы найти минимум  $E(u_x, u_y)$ воспользуемся методом наименьших квадратов, найдем её частные производные по $u_x$ и $u_y$ и запишем в более компактной форме, приравням к 0:

$$
\frac{\partial E(u_x, u_y) }{\partial u_x} = 
\sum \limits_{i,j} g(x_i, y_i) 
\left[
u_x \left( \frac{\partial I}{\partial x} \right)^2 +
	u_y \frac{\partial I}{\partial y}\frac{\partial I}{\partial x} +
	\frac{\partial I}{\partial x}\frac{\partial I}{\partial t}
\right] = 0
$$

$$
\frac{\partial E(u_x, u_y) }{\partial u_y} = 
\sum \limits_{i,j} g(x_i, y_i) 
\left[
	u_x \frac{\partial I}{\partial y}\frac{\partial I}{\partial x} +
	u_y \left( \frac{\partial I}{\partial y} \right)^2 +
	\frac{\partial I}{\partial x}\frac{\partial I}{\partial t}
\right] = 0
$$

Перепишем эти два уравнения в матричной форме:
$$ M \vec{u} = \vec{b}$$

Где: 
$$ M = 
\left[ 	
\begin{array}{cc}
	\sum \limits_{i,j} g(x_i, y_i) 
		\left( \frac{\partial I}{\partial x} \right)^2 
	& \sum \limits_{i,j} g(x_i, y_i) 
			\frac{\partial I}{\partial x} 										\frac{\partial I}{\partial y} 
	\\ 
	\sum \limits_{i,j} g(x_i, y_i) 
			\frac{\partial I}{\partial x} 										\frac{\partial I}{\partial y} 
	& \sum \limits_{i,j} g(x_i, y_i)
		\left( \frac{\partial I}{\partial y} \right)^2 
\end{array} 
\right]
$$

$$
\vec{b} = -
\left[ 	
\begin{array}{c}
\sum \limits_{i,j} g(x_i, y_i) 
			\frac{\partial I}{\partial t} 										\frac{\partial I}{\partial x}  \\ 
\sum \limits_{i,j} g(x_i, y_i) 
			\frac{\partial I}{\partial t} 										\frac{\partial I}{\partial y}
\end{array} 
\right]
$$

$$
\vec{u} = 
\left[ 	
\begin{array}{c}
  u_x\\ 
	u_y
\end{array} 
\right]
$$

Если матрица М обратима (имеет ранг 2), можем вычислить $u_x$ и $u_y$ , которые минимизируют ошибку $E$\cite{habrOpticalFlowAbout}:
$$
\widehat{u} = M^{-1} \cdot \vec{b}
$$

\textbf{Калибровка камеры}
\label{part:calibrateCamera}
Еще одиним математическим аспектом данной работы является калибровка камеры. 

\textit{\textbf{Калибровка камеры}} — это задача получения внутренних и внешних параметров камеры по имеющимся фотографиям или видео, отснятым ей\cite{wikiCalibrate}.

Рассмотрим простейшую модель камеры, камеры-обскуры. В этой простой модели, свет рассматривается в качестве потка от сцены или удаленного объекта, но только один луч попадает из любой конкретный точки этого объекта. Все эти точки проецируются на матрицу камеры или другую поверхность изображения. В результате, изображение на этой плоскости изображения всегда в фокусе, а масштаб изображения относительной его раельного размера определяется одним параметром камеры -  ее фокусным расстоянием. На рисунке~\ref{pic:cameraModel} схематично показана рассматриваемая модель камеры\cite{OpenCVBook}. 

\begin{figure}[!htb]
\center{\includegraphics[width=0.8\linewidth]{pics/cameraModel.png}}
\caption{Модель камеры-обскуры}
\label{pic:cameraModel}
\end{figure}

Из изображения видно, что, на основе подобия треугольников, $-x/f = X/Z$:
$$	-x = f \cdot \frac{X}{Z} $$

Учитывая тот факт, что главная оптическая ось пересекает плоскость изображения не в координате $(0,0)$, в координате $с$ получим:

\begin{equation}
\label{formula:focus}
-x = f \cdot \frac{X}{Z} + с
\end{equation}

Далее будем рассматривать именно цифровые камеры с ПЗС-матрицами. У данных камер есть одна особенность - пиксели матрицы не квадратной формы из-за технологических условий изготовления. Учитывая этот факт, распишем уравнение (\ref{formula:focus}) для координат $x$ и $y$:

$$ u = f \cdot s_u \cdot \frac{X}{Z} + c_u $$
$$ v = f \cdot s_v \cdot \frac{X}{Z} + c_v $$

где $s_u$ и $s_v$ - коэффициенты формы пикселя.
Запишем эти уравнения в матричной форме:

\begin{equation}
\label{formula:matrixProection}
q = \frac{1}{Z} \cdot M \cdot Q
\end{equation}

где 
$Q_i = 
\left( 
\begin{array}{c}
X_i \\ 
Y_i \\ 
Z_i
\end{array} 
\right)$ - координаты точки во внешней системе координат,

$q_i = 
\left( 
\begin{array}{c}
u_i \\ 
v_i \\ 
1
\end{array} 
\right)$ - координаты проекции точки,

$M = 
\left( 
\begin{array}{ccc}
f_u & 0 & c_u \\ 
0 & f_v & c_v \\ 
0 & 0 & 1
\end{array} 
\right)$ - матрица проекции.

Теперь ведем линзу в модель камеры. Линза так же вносит искажения, природа происхождения которых показана на рисунке~\ref{pic:distorb}.

\begin{figure}[!htb]
\center{\includegraphics[width=0.8\linewidth]{pics/distorb.png}}
\caption{Искажения изображения, вносимые линзой}
\label{pic:distorb}
\end{figure}

Данный вид искажения называется радиальным. Он учитывается следующим образом:
$$
u_{corrected} = u \cdot (1 + k_1 \cdot r^2  + k_2 \cdot r^4 )
$$
$$
v_{corrected} = v \cdot (1 + k_1 \cdot r^2  + k_2 \cdot r^4 )
$$
Здесь $r$ – расстояние от точки пересечения главной оптической осью матрицы до точки проекции; $k1, k2$ – коэффициенты радиального искажения.

С учетом этих искажений формула \ref{formula:matrixProection} принимает вид:
\begin{equation}
\label{formula:matrixProection2}
q = \frac{1}{Z} \cdot \left( 
\begin{array}{ccc}
	\lambda & 0 & 0 \\ 
	0 & \lambda & 0 \\ 
	0 & 0 & 1
\end{array} 
\right) 
 \cdot M \cdot Q
\end{equation}
где $\lambda$ - корректирующая функция:
$$ \lambda = 1 + k_1 \cdot r^2  + k_2 \cdot r^4 $$

Кроме радиального искажения существует ещё тангенциальное. Оно возникает из-за того, что плоскость матрицы не перпендикулярна главной оптической оси. Но оно очень мало, и его можно не учитывать. 

При установке камеры тоже появляются погрешности – учтем их в модели.

Погрешности ориентации:
$$R_z(\theta) = 
\left( 
	\begin{array}{ccc}
	\cos (\theta ) & \sin (\theta ) & 0 \\ 
	- \sin (\theta ) & \cos (\theta ) & 0 \\ 
	0 & 0 & 1
\end{array} 
\right) 
$$

$$
R_x(\psi ) = 
\left( 
	\begin{array}{ccc}
	1  & 0 & 0 \\ 
	0 & \cos (\psi ) & \sin (\psi ) \\ 
	0 & - \sin (\psi ) & \cos (\psi ) \\ 
\end{array} 
\right) 
$$

$$
R_y(\varphi ) = 
\left( 
	\begin{array}{ccc}
	\cos (\varphi ) & 0 & - \sin ( \varphi )  \\ 
	0 & 1 & 0 \\ 
	\sin (\varphi ) & 0 & \cos (\varphi ) \\ 
\end{array} 
\right) 
$$

Результирующая матрица вращения камеры вокруг требуемой системы координат:

$$ R(\psi , \varphi , \theta ) = R_x(\psi ) \cdot R_y(\varphi ) \cdot R_z(\theta ) $$

, где $ \theta , \varphi , \psi $ - углы Эйлера.

Вектор смещения камеры относительно требуемой системы координат:
$$ 
T = 
\left( 
\begin{array}{c}
t_x \\ 
t_y \\ 
t_z
\end{array} 
\right) 
$$ 

Теперь модель камеры с учётом радиального искажения и погрешностей установки выглядит так\cite{cameraCalibrate}:
$$
q = \frac{1}{Z} \cdot \left( 
\begin{array}{ccc}
	\lambda & 0 & 0 \\ 
	0 & \lambda & 0 \\ 
	0 & 0 & 1
\end{array} 
\right) 
 \cdot M \cdot R \cdot (Q + T)
$$

\textbf{Вычисление перемещения и угла поворота объекта на основе видеопотока}

Для перехода от векторов оптического потока к векторам перемещения объекта обязательна предварительная калибровка камеры и ее жесткое закреплеине на определнной высоте на носителе. При этом камера закрепляется таким образом, чтобы в кадре 70\% знаимало изображениия поверхности, по которой происходит перемещение. Иными словами, линия горизонта должна находиться на уровне $3/10$ высоты кадра от ее верхнего края. Готовую к визуальной одометрии подвижную платформу схематично можно изобразить так, как показано на рисунке~\ref{pic:objectsSchema}.

\begin{figure}[!h]
\center{\includegraphics[width=0.8\linewidth]{pics/objectsSchema.png}}
\caption{Схема расположения камеры на подвижном объекте.}
\label{pic:objectsSchema}
\end{figure}

После коррекции изображение будет восприниматься так, как будто оно расположено в плоскотси $v$, перпендикулярной основному вектору направления камеры.  Далее это изображение делится на 2 части: часть, отображающую объекты над горизонтом, и часть, отображающую объекты в на плоскости движения. 

Теперь, зная $H$ - высоту закрепления камеры и $D$ - расстояние до пересечения ее главной оптической оси с поверхностью передвижения, можно высчитать полжение точек изображения на поверзности следуюзим образом:
$$
\tan(\alpha ) = H/D,
$$
$$
\tan(\beta ) = (2 v - V) \cdot (VFOF / 2),
$$
где $V$ - высота изображения, $v$ - расстояние от верхнего края изображения до исследуюемой точки изображения, $VFOF$ - вертикальный угол обзора камеры. Тогда:
\begin{equation}\label{formula:y}
y = \frac{H}{\tan (\alpha + \beta) },
\end{equation}

$$
z = \frac{H \cos(\beta)}{\sin (\alpha + \beta)},
$$
где $y$ - расстояние от робота до точки на плоскости, а $z$ - глубина точки (см. рисунок~\ref{pic:objectsSchema}).

Для определения вращения камеры необходимо вычислить расстояние до линии горизонта кадра по формуле~\ref{formula:y}. Если считать, что все точки выше этой линии находятся на таком расстоянии $y_{гор}$, то переходя к цилиндрической системе координат с помошью формулы:
$$
	\varphi = \arctan (\frac{y}{x})
$$
получается разницу между ключевыми точками двух кадров в цилиндрической системе координат в градусах. Эта разница и будет равна углу поворота камеры. 

\subsubsection{Разработка формата и структуры данных}
В ходе функционирования подсистемы информация поступает на ее вход, получается на ее выходе, и передается между модулями внутри системы. 
При этом эта информация носит разные характер и смысл. Так как подсистема носит сугубо программный характер, то все потоки даных реализуются только программными средствами, в виде структур данных. Сетевые технологии не накладывают никаких ограничений на форматы информационных структур. 
Ниже дано описание основных форматов обмена информацией.

\begin{itemize}
\item \textbf{Видео (вход)}. Видео подается на вход системы покадрово, в связи с этим алгоритм кодирования и сжатия видео не существеннен, так как задача сформировать следующий кадр в его полном объеме ложиться на вызывающую стороную. По сути, каждый кадр представляет собой 1 картинку в формате JPEG, что проще всего представать матрицей, размер которой соответствует размеру кадра, а ее элементами являются структуры данных, хранящие информацию о каналах изображения. 

\item \textbf{Ускорения по трем осям (вход)}. Данные подаваемые с акселерометров или одного, трехосевого акселерометра. Данные имеют цифровое представления, дробное. 

\item \textbf{Угловое ускорение по трем осям (вход)}. Данные подаваемые с гироскопов или одного, трехосевого гиросокопа. Данные имеют цифровое представления, дробное. 

\item \textbf{Текущие координаты и угол поворота (выход)}. Высчитанное на текущей итерации положение объекта. Две координаты $X$ и $Y$ представляют собой целые числа. Угол поворота  $\alpha$- дробное число. Аналогиным образом можно представлять информацию о смещениях, полученных на текущей итерации. 

\item \textbf{Информационный обмен между модулем визуальной одометрии и модулем компьютерного зрения}. Данный информационых обмен происходит два раза за итерацию: для нахождения ключевых точек на кадре и для вычисления оптического потока. При этом в разных случаях содержание информационных сообщений будет разное.
	\begin{itemize}
	\item Нахождение ключевых точек - от модуля визуальной одометрии поступает изображение, являющееся кадром. Пресдставляет собой матрицу, размер которой соответствует размеру кадра, а ее элементами являются структуры данных, хранящие информацию о каналах изображения. В ответ модуль компьютерного зрения возвращает список точек в формате ${x, y}$, где $x$ и $y$ - позиция пикселя на изображении при начале отсчета в левом верхнем углу. 
	
	\item Вычисление оптического потока - от модуля визуальной одометрии поступает два изображения (текущий и предыдущий кадры)и список ключевых точек. Представление соответствует описаному ранее. В ответ модуль компьютерного зрения возвращает два списка: список смещений, представляющий собой список пар $\triangle x, \triangle y$, и вектор ошибок, состоящий из 0 и 1 - 1 означает, что соотвествующая ключевая точка не была найдена на втором изображении и вектор смещения найден ошибочно или не найден вообще. 
	
	\end{itemize}
\end{itemize}

\subsubsection{Разработка алгоритмов обработки информации}
\paragraph{Общий алгоритм работы}

Высокоуровнево, алгоритм работы подсистемы выглядит следуюшим образом:
\begin{enumerate}
\item установить текщее положение равным $(0; 0)$, угол, указывающий направление движения, равным $0^{o}$;
\item получить изображение и данные с инерционных измерительных приборов;
\item изменить размер изображения, привести его к нужному формату, устранить искажения в изображении;
\item вычислить перемещения и вращение объекта на основе данных с инерционных измерительных устройств;
\item вычислить перемещения и вращения камеры на основе визуальной одометрии;
\item сравнить данные, полученные на этапах 4 и 5, отбросить менее правдоподобные.
\item прибавить перемещения по осям $oX$ и $oY$, а так же поворот относительно вертикальной оси к предудущим значениям положения и угла направления движения;
\item вернуть данные о текщем положении и оринетации;
\item повторить пункты 2-8 для всех поступающих данных.
\end{enumerate}

Графическое представление общего алгоритма работы в виде блок-схемы предствлено на рисунке~\ref{pic:mainAlgo}.

\begin{figure}[H]
\center{\includegraphics[width=0.45\linewidth]{pics/mainAlgo.png}}
\caption{Блок-схама общего алгоритма работы подсистемы.}
\label{pic:mainAlgo}
\end{figure}

\paragraph{Алгоритм визуальной одометрии}
В общем случае алгоритм визуальной одометрии можно описать следующим образом:
\begin{enumerate}
\item выявить ключевые точки изображения;
\item если нет предыдущего изображения, то сохранить текущее для последующих итераций и перейти к шагу 10;
\item вычилить оптическйи поток;
\item исключение ошибочных векторов;
\item разделение оптического потока на две части: <<наземную>> и <<небесную>>;
\item в <<небесной>> части переход к цилиндрической системе координат и вычисление угла поворота $\theta$ относительно двух последних кадров;
\item в <<наземной>> части выделить векторы  из оптического потока  и вычислить перемещение в плоскости  $x-y$ , получив вектор $(x, y)$;
\item прибавить полученный вектор и угол поворота к изначальному положению объекта, получив новое пложение объекта $(X, Y, \Theta)$;
\item вернуть новое положение объекта;
\item сохранить текущее изображение для следующей итерации;
\item перейти к шагу 1 для следующего кадра;
\end{enumerate}
	
Однако пункт 8 справделив для систем визуальной одометрии в целом, в текущей же работе эта обязанность перекладываются к модулю корректировки выходных данных и всей системы в целом. Алгоритм визуальной одометрии, использующийся в данной работе представлен на рисунке~\ref{pic:visualOdometryAlgo}.

\begin{figure}[H]
\center{\includegraphics[width=0.45\linewidth]{pics/visualalgo.png}}
\caption{Блок-схема алгоритма визуальной одометрии.}
\label{pic:visualOdometryAlgo}
\end{figure}


\paragraph{Алгоритм обработки данных с ИИУ}

Алгоритм обработки данных с инерционных измерительных устройств довольно примитивен и состоит из следующих шагов:
\begin{enumerate}
\item инициация начальных значений: 
	\begin{itemize}
	\item начальная скорость $V_{нач} = V_{пред} = 0$;
	\item начальное ускорение $a_{нач} = 0$;
	\item текущее время $t_0=t_{пред}=0$;
	\item начальный угол направления $ \omega_{пред} = 0$.
	\end{itemize}
\item измерение прошедшего времени $\triangle t = t_{тек}-t_{пред}$;
\item интегрирование данных с акслерометров для получение изменения скоростей и вычисление текущей скорости: 
$$V_{тек} = 
\int_t a + V_{пред} =  
\frac{a_{пред} + a_{тек}}{2} \cdot \triangle t + V_{пред};$$
\item интегрировние полученных скоростей для вычисления перемещения:
$$
S = \frac{V_{пред}+ V_{тек}}{2} \cdot \triangle t;
$$
\item интегрирование данных с гироскопов для получения угла поворота:
$$
\varphi = \frac{\omega_{пред}+ \omega_{тек}}{2} \cdot \triangle t;
$$
\item обновление полученных предыдущих значений текущими;
\item для всех поступаюзих данных повортять пункты 2-6. 
\end{enumerate}

Блок-схема данного алгоритма представлена на рисунке~\ref{pic:IMUAlgo}.
\begin{figure}[H]
\center{\includegraphics[width=0.45\linewidth]{pics/IMUAlgo.png}}
\caption{Блок-схема алгоритма одометрии на основе инерционных измерительных приборов.}
\label{pic:IMUAlgo}
\end{figure}


\paragraph{Алгоритм сопоставления и корректировки выходных данных}
В основе модуля корректировки выходных данных лежит простой алгоритм проверки данных, полученных с помощию визуальной одометрии, путем вычисления сигнальных параметров описаных в пункте~\ref{item:outputCorrect}. 

При <<срабатывании>> хотя бы одного из них, все вышедшие из нормы показатели рассчитываются для данных полученных на основе инерционных измерительных устройств. Далее, из двух вариантов выбирается один, наиболее правдобобный и менее ошибочный. 

Если же не один из сигнальных параметром не сработал, на выход подаются данные, полученные с помощью визуальной одометрии. 

Графиеское представление данного алгоритма в виде блок-схемы представлено на рисунке~\ref{pic:correctAlgo}.

\begin{figure}[H]
\center{\includegraphics[width=0.7\linewidth]{pics/correctAlgo.png}}
\caption{Блок-схема алгоритма модуля корректировки выходных данных.}
\label{pic:correctAlgo}
\end{figure}
